"""
Main FastAPI application for the Translation Web Server.
"""

from fastapi import FastAPI, Request, HTTPException, Depends, Body, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi
import uvicorn
from contextlib import asynccontextmanager
import logging
import time
import asyncio

# Import configuration
from app.config import settings

# Import routers
from app.routers import languages, upload, auth, subscriptions, translate_user, payments, test_helpers, user_transactions, invoices, translation_transactions, company_users, orders, companies, submit
from app.routers import payment_simplified as payment

# Import middleware and utilities
# RateLimitMiddleware removed - causes file upload failures with BaseHTTPMiddleware
# Using slowapi for endpoint-specific rate limiting instead
from app.middleware.logging import LoggingMiddleware
from app.middleware.encoding_fix import EncodingFixMiddleware
from app.middleware.auth_middleware import get_current_user, get_optional_user
from app.utils.health import health_checker

# Import slowapi for endpoint-specific rate limiting
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded


# Application lifespan events
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handle application startup and shutdown events."""
    # Startup
    logging.info(f"Starting {settings.app_name} v{settings.app_version}")

    # Initialize services
    await initialize_services()

    # Create required directories
    settings.ensure_directories()

    yield

    # Shutdown
    logging.info(f"Shutting down {settings.app_name}")
    await cleanup_services()


# Create FastAPI application
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="A comprehensive translation web service supporting multiple languages and file formats",
    docs_url="/docs" if settings.debug else None,
    redoc_url="/redoc" if settings.debug else None,
    openapi_url="/openapi.json" if settings.debug else None,
    lifespan=lifespan
)

# Initialize slowapi rate limiter (endpoint-specific, not global middleware)
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# ============================================================================
# MIDDLEWARE CONFIGURATION - ORDER MATTERS!
# ============================================================================
# FastAPI processes middleware in REVERSE order of addition:
# - Middleware added FIRST executes LAST (closest to endpoint)
# - Middleware added LAST executes FIRST (outermost layer)
#
# Current execution order (request flow):
# 1. timeout_middleware (decorator - outermost)
# 2. LoggingMiddleware
# 3. EncodingFixMiddleware
# 4. RequestBodyDebugMiddleware (for payment endpoints debugging)
# 5. CORSMiddleware (innermost - adds headers last)
# 6. Endpoint
#
# IMPORTANT: CORSMiddleware must be innermost so it processes responses
# from ALL sources including timeout errors from outer middleware
# ============================================================================

# Add custom middleware FIRST (so they execute in the middle)
# DISABLED: RequestBodyDebugMiddleware causes body stream corruption
# app.add_middleware(RequestBodyDebugMiddleware)
app.add_middleware(EncodingFixMiddleware)
app.add_middleware(LoggingMiddleware)
# REMOVED: RateLimitMiddleware - causes file upload failures with BaseHTTPMiddleware
# Using slowapi decorators on specific endpoints instead (see auth.py)

# Add CORS middleware LAST (so it's closest to the endpoint and processes ALL responses)
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=settings.cors_credentials,
    allow_methods=settings.cors_methods,
    allow_headers=["*"] if settings.cors_headers == "*" else settings.cors_headers.split(','),
    expose_headers=["X-Request-ID", "X-Process-Time"]  # Allow frontend to read custom headers
)

# Include routers
app.include_router(languages.router)
app.include_router(upload.router)
app.include_router(submit.router)  # File submission endpoint
app.include_router(payment.router)  # Simplified payment webhooks
app.include_router(payments.router)  # Payment management API
app.include_router(user_transactions.router)  # User transaction payment API
app.include_router(invoices.router)  # Invoice management API
app.include_router(translation_transactions.router)  # Translation transaction management API
app.include_router(company_users.router)  # Company user management API
app.include_router(companies.router)  # Company management API
app.include_router(orders.router)  # Orders management API
app.include_router(auth.router)
app.include_router(subscriptions.router)
app.include_router(translate_user.router)

# Include test helper endpoints only in test/dev mode
if settings.environment.lower() in ["test", "development"]:
    app.include_router(test_helpers.router)
    print("‚ö†Ô∏è  Test helper endpoints enabled (test/dev mode only)")

# Import models for /translate endpoint
from pydantic import BaseModel, EmailStr, Field
from typing import List, Optional, Dict, Any

from app.mongodb_models import TranslationMode

class FileInfo(BaseModel):
    id: str
    name: str
    size: int
    type: str
    content: str  # Base64-encoded file content from client

# Per-file translation mode info (for file-level mode selection)
class FileTranslationModeInfo(BaseModel):
    fileName: str
    translationMode: TranslationMode = TranslationMode.AUTOMATIC

class TranslateRequest(BaseModel):
    files: List[FileInfo]
    fileTranslationModes: Optional[List[FileTranslationModeInfo]] = None  # Per-file translation modes
    sourceLanguage: str
    targetLanguage: str
    email: EmailStr
    paymentIntentId: Optional[str] = None


# ============================================================================
# Transaction Helper Function
# ============================================================================
async def create_transaction_record(
    files_info: List[dict],
    user_data: dict,
    request_data: TranslateRequest,
    subscription: Optional[dict],
    company_name: Optional[str],
    price_per_page: float,
    file_translation_modes: Optional[Dict[str, TranslationMode]] = None  # Per-file translation modes (fileName -> mode)
) -> Optional[str]:
    """
    Create ONE transaction record for MULTIPLE file uploads.

    IMPORTANT: This creates a SINGLE transaction with ALL files in the documents[] array.
    This is the correct behavior - one transaction per upload request, not per file.

    Uses nested documents[] array structure (TranslationDocumentEmbedded model).

    Args:
        files_info: List of dicts, each with file_id, filename, size, page_count, google_drive_url
        user_data: Current authenticated user data
        request_data: Original TranslateRequest
        subscription: Enterprise subscription (or None for individual)
        company_name: Enterprise company name (or None for individual)
        price_per_page: Calculated pricing (subscription or default)
        file_translation_modes: Dict mapping fileName to TranslationMode (per-file modes)

    Returns:
        transaction_id if successful, None if failed
    """
    from app.database import database
    from bson import ObjectId
    from datetime import datetime, timezone
    import uuid

    if not files_info:
        logging.warning("[TRANSACTION] No files provided - skipping transaction creation")
        return None

    try:
        # Generate unique transaction ID
        transaction_id = f"TXN-{uuid.uuid4().hex[:10].upper()}"

        # ====================================================================
        # SINGLE TRANSACTION WITH MULTIPLE DOCUMENTS
        # ====================================================================
        logging.info(f"[TRANSACTION] ========== CREATING SINGLE TRANSACTION ==========")
        logging.info(f"[TRANSACTION] Transaction ID: {transaction_id}")
        logging.info(f"[TRANSACTION] Number of files: {len(files_info)}")
        print(f"\nüì¶ [TRANSACTION] Creating SINGLE transaction {transaction_id} with {len(files_info)} document(s)")

        # Build documents array with ALL files
        documents = []
        total_pages = 0
        file_names = []

        for idx, file_info in enumerate(files_info):
            # Get translation mode for this specific file (default to automatic)
            filename = file_info.get("filename", "")
            file_mode = TranslationMode.AUTOMATIC
            if file_translation_modes and filename in file_translation_modes:
                file_mode = file_translation_modes[filename]

            document_entry = {
                "file_name": filename,
                "file_size": file_info.get("size", 0),
                "original_url": file_info.get("google_drive_url", ""),
                "translated_url": None,
                "translated_name": None,
                "status": "uploaded",
                "uploaded_at": datetime.now(timezone.utc),
                "translated_at": None,
                "processing_started_at": None,
                "processing_duration": None,
                "file_id": file_info.get("file_id"),  # Google Drive file ID for confirm endpoint
                "translation_mode": file_mode.value  # Per-file translation mode
            }
            documents.append(document_entry)
            total_pages += file_info.get("page_count", 1)
            file_names.append(filename or f"file_{idx}")

            logging.info(f"[TRANSACTION]   Document {idx + 1}: {filename} ({file_info.get('page_count', 1)} pages, file_id: {file_info.get('file_id')}, mode: {file_mode.value})")
            print(f"   üìÑ Document {idx + 1}: {filename} ({file_info.get('page_count', 1)} pages, mode: {file_mode.value})")

        # Log transaction-level translation_mode summary
        mode_summary = {}
        for doc in documents:
            mode = doc.get("translation_mode", "automatic")
            mode_summary[mode] = mode_summary.get(mode, 0) + 1
        logging.info(f"[TRANSACTION] Translation mode summary: {mode_summary}")
        print(f"   üéØ Translation modes: {mode_summary}")

        # Build transaction document with nested documents[] array
        # IMPORTANT: Store actual email, not generated user_id
        # NOTE: translation_mode is now per-file (stored in each document entry), not per-transaction
        transaction_doc = {
            "transaction_id": transaction_id,
            "user_id": request_data.email,  # Use actual email for folder lookup
            "source_language": request_data.sourceLanguage,
            "target_language": request_data.targetLanguage,
            "units_count": total_pages,
            "price_per_unit": price_per_page,
            "total_price": total_pages * price_per_page,
            "status": "started",
            "error_message": "",
            "created_at": datetime.now(timezone.utc),
            "updated_at": datetime.now(timezone.utc),
            # NESTED documents[] array - ALL files in ONE transaction (each with translation_mode)
            "documents": documents,
            # Email batching counters
            "total_documents": len(documents),
            "completed_documents": 0,
            "batch_email_sent": False
        }

        # Add enterprise-specific fields if applicable
        if company_name and subscription:
            transaction_doc["company_name"] = company_name  # Store company name for folder lookup
            transaction_doc["subscription_id"] = ObjectId(str(subscription["_id"]))
            transaction_doc["unit_type"] = subscription.get("subscription_unit", "page")
            logging.info(f"[TRANSACTION] Enterprise: {company_name}, Subscription: {subscription.get('_id')}")
        else:
            # Individual customer (no company or subscription)
            transaction_doc["company_name"] = None
            transaction_doc["subscription_id"] = None
            transaction_doc["unit_type"] = "page"
            logging.info(f"[TRANSACTION] Individual customer (no company)")

        # Log the full transaction document before insertion
        import json
        logging.info(f"[TRANSACTION] ========== FULL TRANSACTION DOCUMENT ==========")
        # Create a serializable copy for logging (handle datetime and ObjectId)
        log_doc = {}
        for key, value in transaction_doc.items():
            if key == "documents":
                # Log documents separately with translation_mode highlighted
                log_doc["documents"] = []
                for doc in value:
                    doc_copy = {}
                    for dk, dv in doc.items():
                        if hasattr(dv, 'isoformat'):
                            doc_copy[dk] = dv.isoformat()
                        else:
                            doc_copy[dk] = dv
                    log_doc["documents"].append(doc_copy)
            elif hasattr(value, 'isoformat'):
                log_doc[key] = value.isoformat()
            elif hasattr(value, '__str__') and 'ObjectId' in str(type(value)):
                log_doc[key] = str(value)
            else:
                log_doc[key] = value

        # Print formatted transaction document
        print(f"\n   üìã FULL TRANSACTION DOCUMENT:")
        print(f"   {json.dumps(log_doc, indent=6, default=str)}")
        logging.info(f"[TRANSACTION] Document: {json.dumps(log_doc, default=str)}")
        logging.info(f"[TRANSACTION] ================================================")

        # Insert into appropriate collection based on user type
        if company_name:
            # Enterprise user -> translation_transactions collection
            await database.translation_transactions.insert_one(transaction_doc)
            logging.info(f"[TRANSACTION] üìù Inserted into ENTERPRISE collection: translation_transactions")
            print(f"   üìù Collection: translation_transactions (Enterprise)")
        else:
            # Individual user -> user_transactions collection
            await database.user_transactions.insert_one(transaction_doc)
            logging.info(f"[TRANSACTION] üìù Inserted into INDIVIDUAL collection: user_transactions")
            print(f"   üìù Collection: user_transactions (Individual)")

        logging.info(f"[TRANSACTION] ‚úÖ SUCCESS: Created {transaction_id}")
        logging.info(f"[TRANSACTION]   Total documents: {len(documents)}")
        logging.info(f"[TRANSACTION]   Total pages: {total_pages}")
        logging.info(f"[TRANSACTION]   Total price: ${total_pages * price_per_page:.2f}")
        logging.info(f"[TRANSACTION]   Files: {', '.join(file_names)}")
        logging.info(f"[TRANSACTION] =================================================")

        print(f"   ‚úÖ Transaction {transaction_id} created with {len(documents)} document(s), {total_pages} total pages")
        print(f"   üí∞ Total price: ${total_pages * price_per_page:.2f}")

        return transaction_id

    except Exception as e:
        logging.error(f"[TRANSACTION] ‚ùå FAILED to create transaction: {e}")
        print(f"   ‚ùå [TRANSACTION] Failed to create transaction: {e}")
        return None


# Direct translate endpoint (Google Drive upload)
@app.post("/translate", tags=["Translation"])
async def translate_files(
    request: TranslateRequest = Body(...),
    current_user: Optional[dict] = Depends(get_optional_user)
):
    # DEBUG: This line should appear immediately after Pydantic validation succeeds
    print("üîµ DEBUG: Endpoint function STARTED - Pydantic validation PASSED")
    print(f"üîµ DEBUG: Received {len(request.files)} files")
    print(f"üîµ DEBUG: First file name: {request.files[0].name if request.files else 'NO FILES'}")
    """
    SIMPLIFIED TRANSLATE ENDPOINT:
    1. Upload files to Google Drive {customer_email}/Temp/ folder
    2. Store metadata linking customer to files (no payment sessions)
    3. Return page count for pricing calculation
    4. Frontend processes payment with customer_email
    5. Payment webhook moves files from Temp/ to Inbox/
    """
    # ============================================================================
    # RAW INCOMING DATA - Logged immediately after Pydantic validation
    # ============================================================================
    print("=" * 100)
    print("[RAW INCOMING DATA] /translate ENDPOINT REACHED - Pydantic validation passed")
    print(f"[RAW INCOMING DATA] Authenticated User: {current_user.get('email', 'N/A') if current_user else 'None (Individual User)'}")
    print(f"[RAW INCOMING DATA] Company Name: {current_user.get('company_name', 'N/A') if current_user else 'None (Individual User)'}")
    print(f"[RAW INCOMING DATA] Permission: {current_user.get('permission_level', 'N/A') if current_user else 'None (Individual User)'}")
    print(f"[RAW INCOMING DATA] Request Data:")
    print(f"[RAW INCOMING DATA]   - Customer Email: {request.email}")
    print(f"[RAW INCOMING DATA]   - Source Language: {request.sourceLanguage}")
    print(f"[RAW INCOMING DATA]   - Target Language: {request.targetLanguage}")
    print(f"[RAW INCOMING DATA]   - Number of Files: {len(request.files)}")
    print(f"[RAW INCOMING DATA]   - Payment Intent ID: {request.paymentIntentId or 'None'}")
    print(f"[RAW INCOMING DATA]   - File Translation Modes: {len(request.fileTranslationModes) if request.fileTranslationModes else 0} entries")
    if request.fileTranslationModes:
        for mode_info in request.fileTranslationModes:
            print(f"[RAW INCOMING DATA]     - {mode_info.fileName}: {mode_info.translationMode.value}")
    else:
        print(f"[RAW INCOMING DATA]   ‚ö†Ô∏è NO fileTranslationModes received - will use default (automatic)")
    print(f"[RAW INCOMING DATA] Files Details:")
    for i, file_info in enumerate(request.files, 1):
        print(f"[RAW INCOMING DATA]   File {i}: '{file_info.name}' | {file_info.size:,} bytes | Type: {file_info.type} | ID: {file_info.id}")
    print("=" * 100)

    # Initialize timing tracker
    request_start_time = time.time()

    def log_step(step_name: str, details: str = ""):
        """Log step with timing"""
        elapsed = time.time() - request_start_time
        msg = f"[TRANSLATE {elapsed:6.2f}s] {step_name}"
        if details:
            msg += f" - {details}"
        logging.info(msg)
        print(msg)

    log_step("REQUEST RECEIVED", f"User: {current_user.get('email') if current_user else 'Individual'}, Company: {current_user.get('company_name') if current_user else 'None'}")
    log_step("REQUEST DETAILS", f"Email: {request.email}, {request.sourceLanguage} -> {request.targetLanguage}, Files: {len(request.files)}")

    # Log request payload
    for i, file_info in enumerate(request.files, 1):
        log_step(f"FILE {i} INPUT", f"'{file_info.name}' ({file_info.size:,} bytes, {file_info.type})")

    print(f"TRANSLATE REQUEST RECEIVED")
    print(f"Customer: {request.email}")
    print(f"Translation: {request.sourceLanguage} -> {request.targetLanguage}")
    print(f"Files to process: {len(request.files)}")
    for i, file_info in enumerate(request.files, 1):
        print(f"   File {i}: '{file_info.name}' ({file_info.size:,} bytes, {file_info.type})")

    # Validate email format (additional validation beyond EmailStr)
    log_step("VALIDATION START", "Validating email format")
    import re
    email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$')
    if not email_pattern.match(request.email):
        log_step("VALIDATION FAILED", f"Invalid email format: {request.email}")
        raise HTTPException(
            status_code=400,
            detail="Invalid email format"
        )

    # Check for disposable email domains (stub check)
    disposable_domains = ['tempmail.org', '10minutemail.com', 'guerrillamail.com']
    email_domain = request.email.split('@')[1].lower()
    if email_domain in disposable_domains:
        raise HTTPException(
            status_code=400,
            detail="Disposable email addresses are not allowed"
        )

    # Validate language codes
    valid_languages = [
        'en', 'es', 'fr', 'de', 'it', 'pt', 'ru', 'zh', 'ja', 'ko',
        'ar', 'hi', 'nl', 'pl', 'tr', 'sv', 'da', 'no', 'fi', 'th',
        'vi', 'uk', 'cs', 'hu', 'ro'
    ]

    if request.sourceLanguage not in valid_languages:
        log_step("VALIDATION FAILED", f"Invalid source language: {request.sourceLanguage}")
        raise HTTPException(
            status_code=400,
            detail=f"Invalid source language: {request.sourceLanguage}"
        )

    if request.targetLanguage not in valid_languages:
        log_step("VALIDATION FAILED", f"Invalid target language: {request.targetLanguage}")
        raise HTTPException(
            status_code=400,
            detail=f"Invalid target language: {request.targetLanguage}"
        )

    if request.sourceLanguage == request.targetLanguage:
        log_step("VALIDATION FAILED", "Source and target languages are the same")
        raise HTTPException(
            status_code=400,
            detail="Source and target languages cannot be the same"
        )

    log_step("VALIDATION PASSED", f"Languages: {request.sourceLanguage} -> {request.targetLanguage}")

    # Validate files
    if not request.files:
        log_step("VALIDATION FAILED", "No files provided")
        raise HTTPException(
            status_code=400,
            detail="At least one file is required"
        )

    if len(request.files) > 10:
        log_step("VALIDATION FAILED", f"Too many files: {len(request.files)}")
        raise HTTPException(
            status_code=400,
            detail="Maximum 10 files allowed per request"
        )

    log_step("VALIDATION COMPLETE", f"{len(request.files)} file(s) validated")

    # Import Google Drive service
    from app.services.google_drive_service import google_drive_service
    from app.exceptions.google_drive_exceptions import GoogleDriveError, google_drive_error_to_http_exception

    # Detect customer type (enterprise with company_name vs individual without)
    # IMPORTANT: Do this BEFORE creating folders so we know which structure to create
    company_name = current_user.get("company_name") if current_user else None
    is_enterprise = company_name is not None

    # Enhanced customer type logging
    log_step("CUSTOMER TYPE DETECTED", f"{'Enterprise' if is_enterprise else 'Individual'} (company: {company_name})")
    logging.info(f"[CUSTOMER TYPE] {'‚úì Enterprise' if is_enterprise else '‚óã Individual'}")
    logging.info(f"[CUSTOMER TYPE]   User Email: {request.email}")
    if current_user:
        logging.info(f"[CUSTOMER TYPE]   User ID: {current_user.get('user_id')}")
        logging.info(f"[CUSTOMER TYPE]   User Name: {current_user.get('user_name', 'N/A')}")
    logging.info(f"[CUSTOMER TYPE]   Company Name: {company_name if company_name else 'N/A (individual customer)'}")

    print(f"\nüë§ Customer Type: {'Enterprise' if is_enterprise else 'Individual'}")
    if is_enterprise:
        print(f"   Company: {company_name}")
    user_name = current_user.get('user_name', 'N/A') if current_user else 'N/A'
    print(f"   User: {user_name} ({request.email})")

    # For enterprise users, validate company exists in database
    # CRITICAL: Enforce referential integrity - REJECT if company doesn't exist
    if is_enterprise:
        from app.database import database
        try:
            company_doc = await database.company.find_one({"company_name": company_name})
            if not company_doc:
                # REJECT: Company doesn't exist in database
                log_step("VALIDATION FAILED", f"Company does not exist: {company_name}")
                logging.error(f"[VALIDATION] REJECTED: Company '{company_name}' does not exist in database")
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid company: Company '{company_name}' does not exist in database. Cannot create translation for non-existent company."
                )

            log_step("COMPANY VALIDATED", f"Enterprise: {company_name}")
            print(f"Enterprise company validated: {company_name}")
        except HTTPException:
            raise  # Re-raise HTTP exceptions
        except Exception as e:
            log_step("COMPANY LOOKUP FAILED", f"Error: {e}")
            logging.error(f"[VALIDATION] Database error while validating company: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Database error while validating company: {str(e)}"
            )

    # Create Google Drive folder structure with proper hierarchy
    log_step("FOLDER CREATE START", f"Creating structure for: {request.email}")
    try:
        if is_enterprise and company_name:
            # Enterprise: CompanyName/user_email/Temp/
            print(f"Creating enterprise folder structure: {company_name}/{request.email}/Temp/")
            folder_id = await google_drive_service.create_customer_folder_structure(
                customer_email=request.email,
                company_name=company_name
            )
            log_step("FOLDER CREATED", f"{company_name}/{request.email}/Temp/ (ID: {folder_id})")
            print(f"Google Drive folder created: {company_name}/{request.email}/Temp/ (ID: {folder_id})")
        else:
            # Individual: user_email/Temp/
            print(f"Creating individual folder structure: {request.email}/Temp/")
            folder_id = await google_drive_service.create_customer_folder_structure(
                customer_email=request.email,
                company_name=None
            )
            log_step("FOLDER CREATED", f"{request.email}/Temp/ (ID: {folder_id})")
            print(f"Google Drive folder created: {request.email}/Temp/ (ID: {folder_id})")
    except GoogleDriveError as e:
        log_step("FOLDER CREATE FAILED", f"Google Drive error: {str(e)}")
        print(f"Google Drive error creating folder: {e}")
        raise google_drive_error_to_http_exception(e)
    except Exception as e:
        log_step("FOLDER CREATE FAILED", f"Unexpected error: {str(e)}")
        print(f"Unexpected error creating folder: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to create folder structure: {str(e)}"
        )

    # Initialize pricing variables
    price_per_page = 0.10  # Default for individual customers
    subscription = None

    # Generate storage ID
    import uuid
    storage_id = f"store_{uuid.uuid4().hex[:10]}"

    print(f"Created storage job: {storage_id}")
    if is_enterprise and company_name:
        print(f"Target folder: {company_name}/{request.email}/Temp/ (ID: {folder_id})")
    else:
        print(f"Target folder: {request.email}/Temp/ (ID: {folder_id})")
    print(f"Starting file uploads to Google Drive...")

    # Store files with enhanced metadata (no sessions)
    stored_files = []
    total_pages = 0

    # Pre-build translation modes dict for file upload logging
    upload_file_modes: Dict[str, str] = {}
    if request.fileTranslationModes:
        for mode_info in request.fileTranslationModes:
            upload_file_modes[mode_info.fileName] = mode_info.translationMode.value

    for i, file_info in enumerate(request.files, 1):
        try:
            log_step(f"FILE {i} UPLOAD START", f"'{file_info.name}' ({file_info.size:,} bytes)")
            print(f"   Uploading file {i}/{len(request.files)}: '{file_info.name}'")

            # Decode base64 content from client
            import base64
            try:
                file_content = base64.b64decode(file_info.content)
                log_step(f"FILE {i} BASE64 DECODED", f"Decoded {len(file_content):,} bytes")
            except Exception as e:
                log_step(f"FILE {i} DECODE FAILED", f"Error: {str(e)}")
                raise HTTPException(
                    status_code=400,
                    detail=f"Failed to decode file content for '{file_info.name}': {str(e)}"
                )

            # Count pages for pricing (stub implementation)
            page_count = 1  # Default page count
            if file_info.name.lower().endswith('.pdf'):
                page_count = max(1, file_info.size // 50000)  # Estimate: 50KB per page
            elif file_info.name.lower().endswith(('.doc', '.docx')):
                page_count = max(1, file_info.size // 25000)  # Estimate: 25KB per page

            total_pages += page_count
            log_step(f"FILE {i} PAGE COUNT", f"{page_count} pages estimated")

            # Upload to Google Drive with metadata for customer linking (no sessions)
            log_step(f"FILE {i} GDRIVE UPLOAD", f"Uploading to folder {folder_id}")
            file_result = await google_drive_service.upload_file_to_folder(
                file_content=file_content,  # Use decoded base64 content
                filename=file_info.name,
                folder_id=folder_id,
                target_language=request.targetLanguage
            )
            log_step(f"FILE {i} GDRIVE UPLOADED", f"File ID: {file_result['file_id']}")

            # Get translation_mode for this file BEFORE metadata update (default to automatic)
            # Valid values: automatic, human, formats, handwriting
            file_translation_mode = upload_file_modes.get(file_info.name, "automatic")
            logging.info(f"[FILE {i}] Translation mode assignment: '{file_info.name}' -> '{file_translation_mode}'")

            # Update file with enhanced metadata for payment linking
            log_step(f"FILE {i} METADATA UPDATE", f"Setting file properties (translation_mode: {file_translation_mode})")
            file_metadata = {
                'customer_email': request.email,
                'source_language': request.sourceLanguage,
                'target_language': request.targetLanguage,
                'page_count': str(page_count),
                'status': 'awaiting_payment',
                'upload_timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'original_filename': file_info.name,
                'translation_mode': file_translation_mode  # Added translation_mode to file metadata
            }
            logging.info(f"[FILE {i}] Metadata to be set: {file_metadata}")
            await google_drive_service.update_file_properties(
                file_id=file_result['file_id'],
                properties=file_metadata
            )
            logging.info(f"[FILE {i}] ‚úÖ Metadata SET successfully with translation_mode='{file_translation_mode}'")
            log_step(f"FILE {i} COMPLETE", f"URL: {file_result.get('google_drive_url', 'N/A')}")

            stored_files.append({
                "file_id": file_result['file_id'],
                "filename": file_info.name,
                "status": "stored",
                "page_count": page_count,
                "size": file_info.size,
                "google_drive_url": file_result.get('google_drive_url'),
                "translation_mode": file_translation_mode
            })
            print(f"   Successfully uploaded: '{file_info.name}' -> Google Drive ID: {file_result['file_id']}, Pages: {page_count}, Mode: {file_translation_mode}")

        except Exception as e:
            log_step(f"FILE {i} FAILED", f"Error: {str(e)}")
            print(f"   Failed to upload '{file_info.name}': {e}")
            # Get translation_mode for failed file too (for logging consistency)
            file_translation_mode = upload_file_modes.get(file_info.name, "automatic")
            stored_files.append({
                "file_id": None,
                "filename": file_info.name,
                "status": "failed",
                "page_count": 0,
                "size": file_info.size,
                "error": str(e),
                "translation_mode": file_translation_mode
            })

    # Summary of operation
    successful_uploads = len([f for f in stored_files if f["status"] == "stored"])
    failed_uploads = len([f for f in stored_files if f["status"] == "failed"])

    print(f"UPLOAD COMPLETE: {successful_uploads} successful, {failed_uploads} failed")
    print(f"Total pages for pricing: {total_pages}")
    print(f"Customer: {request.email} (no session needed)")
    print(f"Next step: Process payment, then webhook will move files from Temp to Inbox")

    # For enterprise customers, get subscription pricing
    transaction_ids = []  # Changed from transaction_id to transaction_ids (array)
    if is_enterprise:
        log_step("SUBSCRIPTION QUERY", f"Querying for company: {company_name}")
        logging.info(f"Querying subscription for company: {company_name}")
        from app.services.subscription_service import subscription_service
        from bson import ObjectId
        from datetime import datetime, timezone

        subscriptions = await subscription_service.get_company_subscriptions(
            company_name,
            active_only=True
        )

        if subscriptions and len(subscriptions) > 0:
            subscription = subscriptions[0]
            # Convert MongoDB Decimal128 to float (handles BSON decimal type)
            price_value = subscription.get("price_per_unit", 0.10)
            if hasattr(price_value, 'to_decimal'):  # It's a Decimal128 from MongoDB
                price_per_page = float(price_value.to_decimal())
            else:
                price_per_page = float(price_value)

            # Enhanced subscription logging - Find current active period
            # Database schema: subscription_units, used_units, promotional_units
            # Formula: total_remaining = (subscription_units + promotional_units) - used_units
            usage_periods = subscription.get("usage_periods", [])
            subscription_unit = subscription.get("subscription_unit", "page")
            status = subscription.get("status", "unknown")

            # Find current active period (same logic as subscription_service.py)
            now = datetime.now(timezone.utc)
            current_period = None
            current_period_idx = None

            for idx, period in enumerate(usage_periods):
                period_start = period.get("period_start")
                period_end = period.get("period_end")

                # Handle timezone-aware comparison
                if period_start and not period_start.tzinfo:
                    period_start = period_start.replace(tzinfo=timezone.utc)
                if period_end and not period_end.tzinfo:
                    period_end = period_end.replace(tzinfo=timezone.utc)

                if period_start and period_end and period_start <= now <= period_end:
                    current_period = period
                    current_period_idx = idx
                    break

            # Calculate availability from current active period only
            if current_period:
                subscription_units = current_period.get("subscription_units", 0)
                used_units = current_period.get("used_units", 0)
                promotional_units = current_period.get("promotional_units", 0)
                total_allocated = subscription_units + promotional_units
                total_used = used_units
                total_remaining = total_allocated - used_units
            else:
                # No active period found - cannot use subscription
                total_allocated = 0
                total_used = 0
                total_remaining = 0

            log_step("SUBSCRIPTION FOUND", f"Status: {status}, Price: ${price_per_page} per {subscription_unit}")
            logging.info(f"[SUBSCRIPTION] ‚úì Active subscription found for company {company_name}")
            logging.info(f"[SUBSCRIPTION]   Subscription ID: {subscription.get('_id')}")
            logging.info(f"[SUBSCRIPTION]   Status: {status}")
            logging.info(f"[SUBSCRIPTION]   Price: ${price_per_page} per {subscription_unit}")

            if current_period:
                logging.info(f"[SUBSCRIPTION]   Current Period: {current_period_idx}")
                logging.info(f"[SUBSCRIPTION]   Period Start: {current_period.get('period_start')}")
                logging.info(f"[SUBSCRIPTION]   Period End: {current_period.get('period_end')}")
                logging.info(f"[SUBSCRIPTION]   Subscription Units: {subscription_units}")
                logging.info(f"[SUBSCRIPTION]   Promotional Units: {promotional_units}")
                logging.info(f"[SUBSCRIPTION]   Total Allocated: {total_allocated} {subscription_unit}s")
                logging.info(f"[SUBSCRIPTION]   Used Units: {total_used} {subscription_unit}s")
                logging.info(f"[SUBSCRIPTION]   Remaining Units: {total_remaining} {subscription_unit}s")

                print(f"\nüìä Current Subscription Period:")
                print(f"   Status: {status}")
                print(f"   Period: {current_period_idx} ({current_period.get('period_start')} to {current_period.get('period_end')})")
                print(f"   Subscription Units: {subscription_units} {subscription_unit}s")
                print(f"   Promotional Units: {promotional_units} {subscription_unit}s")
                print(f"   Total Allocated: {total_allocated} {subscription_unit}s")
                print(f"   Used Units: {total_used} {subscription_unit}s")
                print(f"   Remaining Units: {total_remaining} {subscription_unit}s")
                print(f"   Price: ${price_per_page} per {subscription_unit}")
            else:
                logging.warning(f"[SUBSCRIPTION] ‚ö† No active period found for current date")
                print(f"\n‚ö†Ô∏è  No active usage period for current date")
                print(f"   Total usage periods: {len(usage_periods)}")
        else:
            # REJECT: Enterprise user must have an active subscription
            log_step("VALIDATION FAILED", f"No active subscription for company: {company_name}")
            logging.error(f"[SUBSCRIPTION] REJECTED: No active subscription found for company '{company_name}'")
            raise HTTPException(
                status_code=403,
                detail=f"No active subscription found for company '{company_name}'. Enterprise users must have an active subscription to submit translations. Please contact your administrator."
            )

    # Determine if payment is required based on enterprise subscription
    payment_required = True
    if is_enterprise and subscription:
        # Check if enterprise has enough units
        if total_remaining >= total_pages:
            payment_required = False
            log_step("PAYMENT BYPASS", f"Enterprise has {total_remaining} units remaining (needs {total_pages})")
            logging.info(f"[PAYMENT] ‚úì Payment NOT required - using subscription units")
            logging.info(f"[PAYMENT]   Required: {total_pages} {subscription_unit}s")
            logging.info(f"[PAYMENT]   Available: {total_remaining} {subscription_unit}s")
            print(f"\n‚úÖ Payment NOT required - using subscription units")
            print(f"   Required: {total_pages} {subscription_unit}s")
            print(f"   Available: {total_remaining} {subscription_unit}s")
        else:
            log_step("PAYMENT REQUIRED", f"Insufficient units: {total_remaining} remaining, {total_pages} needed")
            logging.warning(f"[PAYMENT] ‚ö† Payment required - insufficient subscription units")
            logging.warning(f"[PAYMENT]   Required: {total_pages} {subscription_unit}s")
            logging.warning(f"[PAYMENT]   Available: {total_remaining} {subscription_unit}s")
            logging.warning(f"[PAYMENT]   Shortfall: {total_pages - total_remaining} {subscription_unit}s")
            print(f"\n‚ö†Ô∏è  Payment required - insufficient subscription units")
            print(f"   Required: {total_pages} {subscription_unit}s")
            print(f"   Available: {total_remaining} {subscription_unit}s")
            print(f"   Shortfall: {total_pages - total_remaining} {subscription_unit}s")
    else:
        log_step("PAYMENT REQUIRED", "Individual customer or no subscription")
        logging.info(f"[PAYMENT] Payment required for {'individual customer' if not is_enterprise else 'enterprise without subscription'}")
        print(f"\nüí≥ Payment required: {'Individual customer' if not is_enterprise else 'Enterprise without subscription'}")

    # ========================================================================
    # CREATE SINGLE TRANSACTION FOR ALL FILES
    # ========================================================================
    # IMPORTANT: One transaction per upload request, NOT one per file!
    # All files are stored in the documents[] array of a single transaction.
    successful_stored_files = [f for f in stored_files if f["status"] == "stored"]

    log_step("TRANSACTION CREATE START", f"Creating SINGLE transaction for {len(successful_stored_files)} file(s)")
    logging.info(f"[TRANSLATE] ========== TRANSACTION CREATION ==========")
    logging.info(f"[TRANSLATE] Creating ONE transaction with {len(successful_stored_files)} document(s)")
    logging.info(f"[TRANSLATE] Files: {[f['filename'] for f in successful_stored_files]}")

    # Build file-level translation modes dict from request
    file_translation_modes: Dict[str, TranslationMode] = {}
    if request.fileTranslationModes:
        for mode_info in request.fileTranslationModes:
            file_translation_modes[mode_info.fileName] = mode_info.translationMode
        logging.info(f"[TRANSLATE] Per-file translation modes: {[(k, v.value) for k, v in file_translation_modes.items()]}")
    else:
        logging.info(f"[TRANSLATE] No per-file modes specified, using default (automatic)")

    if successful_stored_files:
        # Create a SINGLE transaction with ALL files in documents[] array
        transaction_id = await create_transaction_record(
            files_info=successful_stored_files,  # Pass ALL files at once
            user_data=current_user,
            request_data=request,
            subscription=subscription if is_enterprise else None,
            company_name=company_name if is_enterprise else None,
            price_per_page=price_per_page,
            file_translation_modes=file_translation_modes if file_translation_modes else None
        )

        if transaction_id:
            transaction_ids = [transaction_id]  # Single transaction ID
            log_step("TRANSACTION CREATED", f"SINGLE transaction {transaction_id} with {len(successful_stored_files)} document(s)")
            logging.info(f"[TRANSLATE] ‚úÖ SINGLE transaction created: {transaction_id}")
        else:
            logging.error(f"[TRANSLATE] ‚ùå Failed to create transaction")

    logging.info(f"[TRANSLATE] ==============================================")
    log_step("TRANSACTION CREATE COMPLETE", f"Created {len(transaction_ids)} transaction(s) with {len(successful_stored_files)} total document(s)")

    # Extract user information from JWT token (if authenticated)
    user_info = None
    if current_user:
        user_info = {
            "permission_level": current_user.get("permission_level", "user"),
            "email": current_user.get("email", request.email),
            "full_name": current_user.get("fullName") or current_user.get("user_name", "Unknown User")
        }
        log_step("USER INFO", f"Permission: {user_info['permission_level']}, Email: {user_info['email']}, Name: {user_info['full_name']}")
    else:
        # Individual user (not authenticated via corporate login)
        user_info = {
            "permission_level": "user",
            "email": request.email,
            "full_name": "Individual User"
        }
        log_step("USER INFO", "Individual user (no corporate authentication)")

    # Prepare and log response
    log_step("RESPONSE PREPARE", f"Success: {successful_uploads}/{len(request.files)} files, {total_pages} pages, ${total_pages * price_per_page:.2f}")

    response_data = {
        "success": True,
        "data": {
            "id": storage_id,
            "status": "stored",
            "progress": 100,
            "message": f"Files uploaded successfully. Ready for payment.",

                # Pricing information
                "pricing": {
                    "total_pages": total_pages,
                    "price_per_page": price_per_page,  # Dynamic: subscription or default
                    "total_amount": total_pages * price_per_page,  # Dynamic calculation
                    "currency": "USD",
                    "customer_type": "enterprise" if is_enterprise else "individual",
                    "transaction_ids": transaction_ids  # Transaction IDs for all files
                },

                # File information
                "files": {
                    "total_files": len(request.files),
                    "successful_uploads": successful_uploads,
                    "failed_uploads": failed_uploads,
                    "stored_files": stored_files
                },

                # Customer information (used by payment webhook)
                "customer": {
                    "email": request.email,
                    "source_language": request.sourceLanguage,
                    "target_language": request.targetLanguage,
                    "temp_folder_id": folder_id
                },

                # Payment information
                "payment": {
                    "required": payment_required,  # Dynamic: based on subscription units
                    "amount_cents": int(total_pages * price_per_page * 100) if payment_required else 0,  # 0 if using subscription
                    "description": f"Translation service: {successful_uploads} files, {total_pages} pages",
                    "customer_email": request.email
                },

                # User information (permission level and identity)
                "user": user_info
            },
            "error": None
        }

    log_step("RESPONSE SENDING", "Returning response to client")

    # ============================================================================
    # RAW OUTGOING DATA - Logged before sending response
    # ============================================================================
    print("=" * 100)
    print("[RAW OUTGOING DATA] /translate RESPONSE - Sending to client")
    print(f"[RAW OUTGOING DATA] Success: {response_data['success']}")
    print(f"[RAW OUTGOING DATA] Storage ID: {response_data['data']['id']}")
    print(f"[RAW OUTGOING DATA] Status: {response_data['data']['status']}")
    print(f"[RAW OUTGOING DATA] Progress: {response_data['data']['progress']}%")
    print(f"[RAW OUTGOING DATA] Message: {response_data['data']['message']}")
    print(f"[RAW OUTGOING DATA] Pricing:")
    print(f"[RAW OUTGOING DATA]   - Total Pages: {response_data['data']['pricing']['total_pages']}")
    print(f"[RAW OUTGOING DATA]   - Price Per Page: ${response_data['data']['pricing']['price_per_page']}")
    print(f"[RAW OUTGOING DATA]   - Total Amount: ${response_data['data']['pricing']['total_amount']:.2f}")
    print(f"[RAW OUTGOING DATA]   - Currency: {response_data['data']['pricing']['currency']}")
    print(f"[RAW OUTGOING DATA]   - Customer Type: {response_data['data']['pricing']['customer_type']}")
    print(f"[RAW OUTGOING DATA]   - Transaction IDs ({len(response_data['data']['pricing']['transaction_ids'])}): {response_data['data']['pricing']['transaction_ids']}")
    print(f"[RAW OUTGOING DATA] Files:")
    print(f"[RAW OUTGOING DATA]   - Total Files: {response_data['data']['files']['total_files']}")
    print(f"[RAW OUTGOING DATA]   - Successful: {response_data['data']['files']['successful_uploads']}")
    print(f"[RAW OUTGOING DATA]   - Failed: {response_data['data']['files']['failed_uploads']}")
    for i, file in enumerate(response_data['data']['files']['stored_files'], 1):
        print(f"[RAW OUTGOING DATA]   File {i}: '{file['filename']}' | Status: {file['status']} | Pages: {file['page_count']} | Mode: {file.get('translation_mode', 'automatic')} | GDrive: {file.get('google_drive_url', 'N/A')[:50]}...")
    print(f"[RAW OUTGOING DATA] Customer: {response_data['data']['customer']['email']}")
    print(f"[RAW OUTGOING DATA] Payment Required: {response_data['data']['payment']['required']}")
    print(f"[RAW OUTGOING DATA] Payment Amount (cents): {response_data['data']['payment']['amount_cents']}")
    print(f"[RAW OUTGOING DATA] User Information:")
    print(f"[RAW OUTGOING DATA]   - Permission Level: {response_data['data']['user']['permission_level']}")
    print(f"[RAW OUTGOING DATA]   - Email: {response_data['data']['user']['email']}")
    print(f"[RAW OUTGOING DATA]   - Full Name: {response_data['data']['user']['full_name']}")
    print("=" * 100)

    return JSONResponse(content=response_data)


# ============================================================================
# Transaction Confirmation/Decline Models
# ============================================================================
class TransactionActionRequest(BaseModel):
    transaction_ids: List[str]


class EnterpriseConfirmRequest(BaseModel):
    """Enterprise confirmation - uses transaction_id only, no file search"""
    transaction_id: str = Field(..., description="Transaction ID from upload response")
    status: bool = Field(default=True, description="True=confirm, False=cancel")


class IndividualConfirmRequest(BaseModel):
    """Individual confirmation - includes payment info and file_ids from upload"""
    transaction_id: str = Field(..., description="Transaction ID from upload response")
    square_transaction_id: Optional[str] = Field(None, description="Square payment transaction ID")
    file_ids: List[str] = Field(..., description="File IDs from upload response - NO SEARCH")
    status: bool = Field(default=True, description="True=confirm, False=cancel")


# ============================================================================
# Background Task for Transaction Confirmation
# ============================================================================
async def process_transaction_confirmation_background(
    transaction_ids: List[str],
    customer_email: str,
    company_name: Optional[str],
    file_ids: List[str]
):
    """
    Background task to move files from Temp to Inbox and update transaction status.
    Runs asynchronously without blocking HTTP response.
    """
    import time
    task_start = time.time()

    try:
        from app.database import database
        from app.services.google_drive_service import google_drive_service
        from app.services.subscription_service import subscription_service
        from app.models.subscription import UsageUpdate
        from datetime import datetime, timezone

        print("\n" + "=" * 80)
        print("üîÑ BACKGROUND TASK STARTED - Transaction Confirmation")
        print("=" * 80)
        print(f"‚è±Ô∏è  Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üìã Task Details:")
        print(f"   Customer: {customer_email}")
        print(f"   Company: {company_name or 'Individual'}")
        print(f"   Transactions: {len(transaction_ids)}")
        print(f"   Files to move: {len(file_ids)}")
        print("=" * 80)

        # Move files from Temp to Inbox
        print(f"\nüìÅ Step 1: Moving files from Temp to Inbox...")
        move_start = time.time()
        move_result = await google_drive_service.move_files_to_inbox_on_payment_success(
            customer_email=customer_email,
            file_ids=file_ids,
            company_name=company_name
        )
        move_time = (time.time() - move_start) * 1000
        print(f"‚è±Ô∏è  File move completed in {move_time:.2f}ms")
        print(f"‚úÖ Moved: {move_result['moved_successfully']}/{move_result['total_files']} files")

        # Verify files in Inbox
        print(f"\nüîç Step 2: Verifying files in Inbox...")
        inbox_folder_id = move_result['inbox_folder_id']
        verified_count = 0

        for moved_file in move_result.get('moved_files', []):
            file_id = moved_file['file_id']
            try:
                file_info_raw = await asyncio.to_thread(
                    lambda: google_drive_service.service.files().get(
                        fileId=file_id,
                        fields='id,name,parents'
                    ).execute()
                )
                current_parents = file_info_raw.get('parents', [])
                is_in_inbox = inbox_folder_id in current_parents

                if is_in_inbox:
                    verified_count += 1
                    print(f"   ‚úÖ File {file_id[:20]}... verified in Inbox")
                else:
                    print(f"   ‚ö†Ô∏è  File {file_id[:20]}... NOT in Inbox")

            except Exception as e:
                print(f"   ‚ùå Failed to verify file {file_id[:20]}...: {e}")

        print(f"‚úÖ Verified: {verified_count}/{len(move_result.get('moved_files', []))} files")

        # Step 5: Update file properties with transaction_id
        print(f"\nüìù Step 5: Adding transaction IDs to file metadata...")
        metadata_start = time.time()
        metadata_updates_successful = 0

        for i, file_id in enumerate(file_ids):
            try:
                transaction_id = transaction_ids[i]
                await google_drive_service.update_file_properties(
                    file_id=file_id,
                    properties={
                        'transaction_id': transaction_id,
                        'status': 'confirmed',
                        'confirmation_timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
                    }
                )
                metadata_updates_successful += 1
                print(f"   ‚úì File {file_id[:20]}...: Added transaction_id={transaction_id}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Failed to update metadata for file {file_id[:20]}...: {e}")
                # Continue processing other files even if one fails

        metadata_elapsed = (time.time() - metadata_start) * 1000
        print(f"‚è±Ô∏è  Metadata update completed in {metadata_elapsed:.2f}ms")
        print(f"‚úÖ Updated: {metadata_updates_successful}/{len(file_ids)} files")

        # Update transaction status
        print(f"\nüîÑ Step 3: Updating transaction status...")
        for txn_id in transaction_ids:
            await database.translation_transactions.update_one(
                {"transaction_id": txn_id},
                {
                    "$set": {
                        "status": "confirmed",
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            print(f"   ‚úì Transaction {txn_id} confirmed")

        # Update subscription usage
        print(f"\nüí≥ Step 4: Updating subscription usage...")
        transactions = []
        for txn_id in transaction_ids:
            txn = await database.translation_transactions.find_one({"transaction_id": txn_id})
            if txn:
                transactions.append(txn)

        subscription_updates = {}
        for txn in transactions:
            subscription_id = txn.get("subscription_id")
            units_count = txn.get("units_count", 0)

            if subscription_id and units_count > 0:
                sub_id_str = str(subscription_id)
                if sub_id_str not in subscription_updates:
                    subscription_updates[sub_id_str] = 0
                subscription_updates[sub_id_str] += units_count

        if subscription_updates:
            for subscription_id_str, total_units in subscription_updates.items():
                try:
                    usage_update = UsageUpdate(
                        units_to_add=total_units,
                        use_promotional_units=False
                    )
                    await subscription_service.record_usage(
                        subscription_id=subscription_id_str,
                        usage_data=usage_update
                    )
                    print(f"   ‚úÖ Subscription {subscription_id_str}: +{total_units} units")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Subscription update failed for {subscription_id_str}: {e}")
        else:
            print(f"   ‚ÑπÔ∏è  No subscription updates needed (individual customer)")

        total_time = (time.time() - task_start) * 1000
        print(f"\n‚úÖ BACKGROUND TASK COMPLETE")
        print(f"‚è±Ô∏è  TOTAL TASK TIME: {total_time:.2f}ms")
        print(f"üìä Results: {verified_count}/{len(file_ids)} files verified in Inbox")
        print("=" * 80 + "\n")

        logging.info(f"Background task completed for {customer_email}: {verified_count}/{len(file_ids)} files verified in {total_time:.2f}ms")

    except Exception as e:
        total_time = (time.time() - task_start) * 1000
        print(f"\n‚ùå BACKGROUND TASK ERROR")
        print(f"‚è±Ô∏è  Failed after: {total_time:.2f}ms")
        print(f"üí• Error type: {type(e).__name__}")
        print(f"üí• Error message: {str(e)}")
        print("=" * 80 + "\n")
        logging.error(f"Background confirmation error for {customer_email}: {e}", exc_info=True)


# ============================================================================
# Transaction Confirmation Endpoint
# ============================================================================
@app.post("/api/transactions/confirm", tags=["Transactions"])
async def confirm_transactions(
    request: TransactionActionRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """
    Confirm transactions and move files from Temp/ to Inbox/.

    This endpoint:
    1. Updates transaction status from "started" to "confirmed"
    2. Moves files from customer_email/Temp/ to customer_email/Inbox/
    3. Returns count of successfully moved files
    """
    print(f"[CONFIRM ENDPOINT] ‚úÖ REACHED! Pydantic body parsing completed successfully!")
    print(f"[CONFIRM ENDPOINT] Transaction IDs: {request.transaction_ids}")
    print(f"[CONFIRM ENDPOINT] Current user: {current_user.get('email')}")

    from app.database import database
    from app.services.google_drive_service import google_drive_service
    from datetime import datetime, timezone

    logging.info(f"[CONFIRM] User {current_user.get('email')} confirming {len(request.transaction_ids)} transactions")
    print(f"[CONFIRM] Confirming transactions: {request.transaction_ids}")

    if not request.transaction_ids:
        raise HTTPException(status_code=400, detail="No transaction IDs provided")

    try:
        # Fetch all transactions
        transactions = []
        print(f"[CONFIRM DEBUG] Fetching {len(request.transaction_ids)} transaction(s) from database...")
        for txn_id in request.transaction_ids:
            txn = await database.translation_transactions.find_one({"transaction_id": txn_id})
            if txn:
                transactions.append(txn)
                print(f"[CONFIRM DEBUG] Found transaction {txn_id}")
                print(f"[CONFIRM DEBUG]   - user_id: {txn.get('user_id', 'MISSING')}")
                print(f"[CONFIRM DEBUG]   - original_file_url: {txn.get('original_file_url', 'MISSING')}")
                print(f"[CONFIRM DEBUG]   - file_name: {txn.get('file_name', 'MISSING')}")
                print(f"[CONFIRM DEBUG]   - status: {txn.get('status', 'MISSING')}")
            else:
                logging.warning(f"[CONFIRM] Transaction {txn_id} not found")
                print(f"[CONFIRM DEBUG] ‚ùå Transaction {txn_id} NOT FOUND in database")

        if not transactions:
            raise HTTPException(status_code=404, detail="No valid transactions found")

        # Extract customer email and company name from first transaction
        first_txn = transactions[0]
        customer_email = first_txn.get("user_id", "")  # user_id contains email
        company_name = first_txn.get("company_name")  # company_name for enterprise customers
        print(f"[CONFIRM DEBUG] Customer email extracted: {customer_email}")
        print(f"[CONFIRM DEBUG] Company name extracted: {company_name or 'None (individual customer)'}")

        # Get file IDs from original_file_url (Google Drive URLs contain file ID)
        file_ids = []
        print(f"[CONFIRM DEBUG] Extracting file IDs from {len(transactions)} transaction(s)...")
        for i, txn in enumerate(transactions, 1):
            url = txn.get("original_file_url", "")
            print(f"[CONFIRM DEBUG] Transaction {i}/{len(transactions)}: URL = '{url}'")

            # Extract file ID from Google Drive URL - handles multiple formats:
            # - Google Docs: https://docs.google.com/document/d/{file_id}/edit
            # - Google Sheets: https://docs.google.com/spreadsheets/d/{file_id}/edit
            # - Google Drive: https://drive.google.com/file/d/{file_id}/view
            file_id = None
            if "/document/d/" in url:
                file_id = url.split("/document/d/")[1].split("/")[0]
                print(f"[CONFIRM DEBUG]   ‚úÖ Extracted file_id from /document/d/ pattern: {file_id}")
            elif "/spreadsheets/d/" in url:
                file_id = url.split("/spreadsheets/d/")[1].split("/")[0]
                print(f"[CONFIRM DEBUG]   ‚úÖ Extracted file_id from /spreadsheets/d/ pattern: {file_id}")
            elif "/file/d/" in url:
                file_id = url.split("/file/d/")[1].split("/")[0]
                print(f"[CONFIRM DEBUG]   ‚úÖ Extracted file_id from /file/d/ pattern: {file_id}")
            else:
                print(f"[CONFIRM DEBUG]   ‚ùå URL does not match any known Google Drive pattern - SKIPPED")

            if file_id:
                file_ids.append(file_id)

        print(f"[CONFIRM DEBUG] Total file IDs extracted: {len(file_ids)}")
        print(f"[CONFIRM DEBUG] File IDs: {file_ids}")

        # Log what we're about to do (fast - logging only)
        if company_name:
            logging.info(f"[CONFIRM] Scheduling background task to move {len(file_ids)} files from {company_name}/{customer_email}/Temp/ to Inbox/")
            print(f"[CONFIRM] Scheduling background task: {len(file_ids)} files from {company_name}/{customer_email}/Temp/ to Inbox/")
        else:
            logging.info(f"[CONFIRM] Scheduling background task to move {len(file_ids)} files from {customer_email}/Temp/ to Inbox/")
            print(f"[CONFIRM] Scheduling background task: {len(file_ids)} files from {customer_email}/Temp/ to Inbox/")

        # Schedule background task for file move, verification, and updates
        print(f"[CONFIRM] Adding background task to queue...")
        background_tasks.add_task(
            process_transaction_confirmation_background,
            transaction_ids=request.transaction_ids,
            customer_email=customer_email,
            company_name=company_name,
            file_ids=file_ids
        )
        print(f"[CONFIRM] ‚úÖ Background task scheduled successfully")

        # Return IMMEDIATELY (< 1 second) - background task will handle everything else
        response = {
            "success": True,
            "message": f"Transactions confirmed. Files are being moved to Inbox in the background.",
            "data": {
                "confirmed_transactions": len(request.transaction_ids),
                "total_files": len(file_ids),
                "status": "processing",
                "customer_email": customer_email,
                "company_name": company_name if company_name else None,
                "processing_note": "Files are being moved and verified in the background. This may take 30-90 seconds to complete."
            }
        }

        logging.info(f"[CONFIRM] Immediate response sent: {len(request.transaction_ids)} transactions scheduled for processing")
        print(f"[CONFIRM] ‚ö° INSTANT RESPONSE: {len(request.transaction_ids)} transactions scheduled (background processing started)")

        return JSONResponse(content=response)

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"[CONFIRM] Failed to confirm transactions: {e}", exc_info=True)
        print(f"[CONFIRM] Error: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to confirm transactions: {str(e)}"
        )


# ============================================================================
# Transaction Decline Endpoint
# ============================================================================
@app.post("/api/transactions/decline", tags=["Transactions"])
async def decline_transactions(
    request: TransactionActionRequest = Body(...),
    current_user: dict = Depends(get_current_user)
):
    """
    Decline transactions and delete files from Temp/.

    This endpoint:
    1. Updates transaction status from "started" to "declined"
    2. Deletes files from customer_email/Temp/ folder
    3. Returns count of successfully deleted files
    """
    from app.database import database
    from app.services.google_drive_service import google_drive_service
    from datetime import datetime, timezone

    logging.info(f"[DECLINE] User {current_user.get('email')} declining {len(request.transaction_ids)} transactions")
    print(f"[DECLINE] Declining transactions: {request.transaction_ids}")

    if not request.transaction_ids:
        raise HTTPException(status_code=400, detail="No transaction IDs provided")

    try:
        # Fetch all transactions
        transactions = []
        for txn_id in request.transaction_ids:
            txn = await database.translation_transactions.find_one({"transaction_id": txn_id})
            if txn:
                transactions.append(txn)
            else:
                logging.warning(f"[DECLINE] Transaction {txn_id} not found")

        if not transactions:
            raise HTTPException(status_code=404, detail="No valid transactions found")

        # Extract customer email from first transaction
        first_txn = transactions[0]
        customer_email = first_txn.get("user_id", "")  # user_id contains email

        # Get file IDs from original_file_url (Google Drive URLs contain file ID)
        file_ids = []
        for txn in transactions:
            url = txn.get("original_file_url", "")
            # Extract file ID from Google Drive URL - handles multiple formats:
            # - Google Docs: https://docs.google.com/document/d/{file_id}/edit
            # - Google Sheets: https://docs.google.com/spreadsheets/d/{file_id}/edit
            # - Google Drive: https://drive.google.com/file/d/{file_id}/view
            file_id = None
            if "/document/d/" in url:
                file_id = url.split("/document/d/")[1].split("/")[0]
            elif "/spreadsheets/d/" in url:
                file_id = url.split("/spreadsheets/d/")[1].split("/")[0]
            elif "/file/d/" in url:
                file_id = url.split("/file/d/")[1].split("/")[0]

            if file_id:
                file_ids.append(file_id)

        logging.info(f"[DECLINE] Deleting {len(file_ids)} files from Temp/ for {customer_email}")
        print(f"[DECLINE] Deleting {len(file_ids)} files from Temp/")

        # Delete files using existing Google Drive service function
        deleted_count = await google_drive_service.delete_files_on_payment_failure(
            customer_email=customer_email,
            file_ids=file_ids
        )

        # Update transaction status to "declined"
        for txn_id in request.transaction_ids:
            await database.translation_transactions.update_one(
                {"transaction_id": txn_id},
                {
                    "$set": {
                        "status": "declined",
                        "error_message": "User declined the transaction",
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            logging.info(f"[DECLINE] Transaction {txn_id} status updated to 'declined'")
            print(f"[DECLINE] Transaction {txn_id} declined")

        response = {
            "success": True,
            "message": f"Successfully declined {len(request.transaction_ids)} transaction(s)",
            "data": {
                "declined_transactions": len(request.transaction_ids),
                "deleted_files": deleted_count
            }
        }

        logging.info(f"[DECLINE] Success: {len(request.transaction_ids)} transactions declined, {deleted_count} files deleted")
        print(f"[DECLINE] Complete: {len(request.transaction_ids)} transactions declined, {deleted_count} files deleted")

        return JSONResponse(content=response)

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"[DECLINE] Failed to decline transactions: {e}", exc_info=True)
        print(f"[DECLINE] Error: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to decline transactions: {str(e)}"
        )


# ============================================================================
# Enterprise Transaction Confirmation Endpoint
# ============================================================================
@app.post("/api/transactions/confirm-enterprise", tags=["Transactions"])
async def confirm_enterprise_transaction(
    request: EnterpriseConfirmRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    Confirm Enterprise transaction (no payment, no file search).

    This endpoint:
    - Validates user is Enterprise (has company_name in JWT)
    - Finds transaction by transaction_id in translation_transactions collection
    - Verifies transaction belongs to user's company
    - Gets file_ids from transaction.documents field (NO SEARCH)
    - If status=True: Moves files Temp ‚Üí Inbox, updates transaction to 'processing'
    - If status=False: Deletes files from Temp, updates transaction to 'cancelled'
    """
    import json
    from app.database import database
    from app.services.google_drive_service import google_drive_service
    from datetime import datetime, timezone

    logging.info(f"[CONFIRM-ENTERPRISE] Request received for transaction {request.transaction_id}")

    # Extract user info from JWT
    user_email = current_user.get("email")
    company_name = current_user.get("company_name")

    # Validate user is Enterprise
    if not company_name:
        logging.error(f"[CONFIRM-ENTERPRISE] User {user_email} is not Enterprise (no company_name)")
        raise HTTPException(
            status_code=403,
            detail="This endpoint is only for Enterprise users"
        )

    logging.info(f"[CONFIRM-ENTERPRISE] User {user_email}, Company {company_name}")

    try:
        # Find transaction by transaction_id
        transaction = await database.translation_transactions.find_one({
            "transaction_id": request.transaction_id
        })

        if not transaction:
            logging.error(f"[CONFIRM-ENTERPRISE] Transaction {request.transaction_id} not found")
            raise HTTPException(
                status_code=404,
                detail=f"Transaction {request.transaction_id} not found"
            )

        # Verify transaction belongs to user's company
        if transaction.get("company_name") != company_name:
            logging.error(
                f"[CONFIRM-ENTERPRISE] Transaction {request.transaction_id} belongs to "
                f"{transaction.get('company_name')}, not {company_name}"
            )
            raise HTTPException(
                status_code=403,
                detail="Transaction does not belong to your company"
            )

        # Get file_ids from transaction.documents (NO SEARCH)
        documents = transaction.get("documents", [])

        logging.info(f"[CONFIRM-ENTERPRISE] Transaction ID: {transaction.get('transaction_id')}, Status: {transaction.get('status')}")
        logging.info(f"[CONFIRM-ENTERPRISE] Documents array: {json.dumps(documents, indent=2, default=str)}")

        if not documents:
            logging.error(f"[CONFIRM-ENTERPRISE] Transaction has no documents array")
            logging.error(f"[CONFIRM-ENTERPRISE] Available keys: {list(transaction.keys())}")
            raise HTTPException(
                status_code=400,
                detail="Transaction has no documents"
            )

        file_ids = [doc.get("file_id") for doc in documents if doc.get("file_id")]

        logging.info(f"[CONFIRM-ENTERPRISE] Extracted file_ids: {file_ids}")
        logging.info(f"[CONFIRM-ENTERPRISE] File_ids count: {len(file_ids)}")

        if not file_ids:
            logging.error(f"[CONFIRM-ENTERPRISE] No file_ids found in documents")
            logging.error(f"[CONFIRM-ENTERPRISE] Documents structure: {json.dumps(documents, indent=2, default=str)}")
            raise HTTPException(
                status_code=400,
                detail="Transaction documents have no file_ids"
            )

        logging.info(f"[CONFIRM-ENTERPRISE] Found {len(file_ids)} files in transaction")

        # ========== SUCCESS FLOW: Confirm ==========
        if request.status:
            logging.info(f"[CONFIRM-ENTERPRISE] Confirming transaction {request.transaction_id}")

            # Update transaction status to 'processing'
            await database.translation_transactions.update_one(
                {"transaction_id": request.transaction_id},
                {
                    "$set": {
                        "status": "processing",
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            logging.info(f"[CONFIRM-ENTERPRISE] Updated transaction status to 'processing'")

            # Update file metadata with transaction_id
            for file_id in file_ids:
                try:
                    await google_drive_service.update_file_metadata(
                        file_id=file_id,
                        metadata={"transaction_id": request.transaction_id}
                    )
                    logging.info(f"[CONFIRM-ENTERPRISE] Updated metadata for file {file_id}")
                except Exception as e:
                    logging.error(f"[CONFIRM-ENTERPRISE] Failed to update metadata for {file_id}: {e}")

            # Move files from Temp to Inbox
            try:
                customer_email = transaction.get("user_id")  # user_id contains the email address
                move_result = await google_drive_service.move_files_to_inbox_on_payment_success(
                    customer_email=customer_email,
                    file_ids=file_ids,
                    company_name=company_name
                )

                moved_count = move_result.get('moved_successfully', 0)
                failed_count = move_result.get('failed_moves', 0)

                logging.info(f"[CONFIRM-ENTERPRISE] Moved {moved_count}/{len(file_ids)} files successfully")

                if failed_count > 0:
                    logging.warning(f"[CONFIRM-ENTERPRISE] {failed_count} files failed to move")

                # Update subscription usage
                subscription_id = transaction.get("subscription_id")
                units_count = transaction.get("units_count", 0)

                logging.info(f"[CONFIRM-ENTERPRISE] Subscription tracking - ID: {subscription_id}, Units: {units_count}")

                if subscription_id and units_count > 0:
                    from app.services.subscription_service import subscription_service
                    from app.models.subscription import UsageUpdate

                    try:
                        usage_update = UsageUpdate(
                            units_to_add=units_count,
                            use_promotional_units=False
                        )
                        await subscription_service.record_usage(
                            subscription_id=str(subscription_id),
                            usage_data=usage_update
                        )
                        logging.info(f"[CONFIRM-ENTERPRISE] Updated subscription usage: +{units_count} units")
                    except Exception as e:
                        logging.error(f"[CONFIRM-ENTERPRISE] Subscription update failed: {e}")
                        # Don't fail the entire operation if usage tracking fails
                        # The files were moved successfully, so we should return success
                else:
                    logging.warning(
                        f"[CONFIRM-ENTERPRISE] Skipping subscription update - "
                        f"subscription_id: {subscription_id}, units_count: {units_count}"
                    )

                return JSONResponse(
                    status_code=200,
                    content={
                        "success": True,
                        "message": "Enterprise transaction confirmed successfully",
                        "data": {
                            "transaction_id": request.transaction_id,
                            "moved_files": moved_count,
                            "failed_files": failed_count,
                            "total_files": len(file_ids),
                            "company_name": company_name
                        }
                    }
                )

            except Exception as e:
                logging.error(f"[CONFIRM-ENTERPRISE] File movement failed: {e}", exc_info=True)
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to move files: {str(e)}"
                )

        # ========== CANCEL FLOW: Delete files ==========
        else:
            logging.info(f"[CONFIRM-ENTERPRISE] Cancelling transaction {request.transaction_id}")

            # Update transaction status to 'cancelled'
            await database.translation_transactions.update_one(
                {"transaction_id": request.transaction_id},
                {
                    "$set": {
                        "status": "cancelled",
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            logging.info(f"[CONFIRM-ENTERPRISE] Updated transaction status to 'cancelled'")

            # Delete files from Temp
            deleted_count = 0
            for file_id in file_ids:
                try:
                    await google_drive_service.delete_file(file_id)
                    deleted_count += 1
                    logging.info(f"[CONFIRM-ENTERPRISE] Deleted file {file_id}")
                except Exception as e:
                    logging.error(f"[CONFIRM-ENTERPRISE] Failed to delete file {file_id}: {e}")

            logging.info(f"[CONFIRM-ENTERPRISE] Deleted {deleted_count}/{len(file_ids)} files")

            return JSONResponse(
                status_code=200,
                content={
                    "success": True,
                    "message": "Enterprise transaction cancelled, files deleted",
                    "data": {
                        "transaction_id": request.transaction_id,
                        "deleted_files": deleted_count,
                        "total_files": len(file_ids),
                        "company_name": company_name
                    }
                }
            )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"[CONFIRM-ENTERPRISE] Unexpected error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )


# ============================================================================
# Individual Transaction Confirmation Endpoint
# ============================================================================
@app.post("/api/transactions/confirm-individual", tags=["Transactions"])
async def confirm_individual_transaction(
    request: IndividualConfirmRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    Confirm Individual transaction with payment (no file search).

    This endpoint:
    - Validates user is Individual (no company_name in JWT)
    - Finds transaction by transaction_id in user_transactions collection
    - Verifies transaction belongs to current user
    - Uses file_ids from request (NO SEARCH)
    - If status=True: Stores square_transaction_id, moves files Temp ‚Üí Inbox, updates transaction to 'completed'
    - If status=False: Deletes files from Temp, updates transaction to 'cancelled'
    """
    from app.database import database
    from app.services.google_drive_service import google_drive_service
    from datetime import datetime, timezone

    logging.info(f"[CONFIRM-INDIVIDUAL] Request received for transaction {request.transaction_id}")

    # Extract user info from JWT
    user_email = current_user.get("email")
    company_name = current_user.get("company_name")

    # Validate user is Individual (no company_name)
    if company_name:
        logging.error(f"[CONFIRM-INDIVIDUAL] User {user_email} is Enterprise (has company_name: {company_name})")
        raise HTTPException(
            status_code=403,
            detail="This endpoint is only for Individual users"
        )

    logging.info(f"[CONFIRM-INDIVIDUAL] User {user_email}")

    try:
        # Find transaction by transaction_id
        transaction = await database.user_transactions.find_one({
            "transaction_id": request.transaction_id
        })

        if not transaction:
            logging.error(f"[CONFIRM-INDIVIDUAL] Transaction {request.transaction_id} not found")
            raise HTTPException(
                status_code=404,
                detail=f"Transaction {request.transaction_id} not found"
            )

        # Verify transaction belongs to current user
        if transaction.get("user_email") != user_email:
            logging.error(
                f"[CONFIRM-INDIVIDUAL] Transaction {request.transaction_id} belongs to "
                f"{transaction.get('user_email')}, not {user_email}"
            )
            raise HTTPException(
                status_code=403,
                detail="Transaction does not belong to your account"
            )

        # Use file_ids from request (NO SEARCH)
        file_ids = request.file_ids
        if not file_ids:
            logging.error(f"[CONFIRM-INDIVIDUAL] No file_ids provided in request")
            raise HTTPException(
                status_code=400,
                detail="file_ids are required"
            )

        logging.info(f"[CONFIRM-INDIVIDUAL] Processing {len(file_ids)} files from request")

        # ========== SUCCESS FLOW: Confirm with payment ==========
        if request.status:
            logging.info(f"[CONFIRM-INDIVIDUAL] Confirming transaction {request.transaction_id}")

            # Update transaction with square_transaction_id and status
            update_data = {
                "status": "completed",
                "updated_at": datetime.now(timezone.utc)
            }
            if request.square_transaction_id:
                update_data["square_transaction_id"] = request.square_transaction_id
                logging.info(f"[CONFIRM-INDIVIDUAL] Storing Square transaction ID: {request.square_transaction_id}")

            await database.user_transactions.update_one(
                {"transaction_id": request.transaction_id},
                {"$set": update_data}
            )
            logging.info(f"[CONFIRM-INDIVIDUAL] Updated transaction status to 'completed'")

            # Update file metadata with transaction_id
            for file_id in file_ids:
                try:
                    await google_drive_service.update_file_metadata(
                        file_id=file_id,
                        metadata={"transaction_id": request.transaction_id}
                    )
                    logging.info(f"[CONFIRM-INDIVIDUAL] Updated metadata for file {file_id}")
                except Exception as e:
                    logging.error(f"[CONFIRM-INDIVIDUAL] Failed to update metadata for {file_id}: {e}")

            # Move files from Temp to Inbox
            try:
                move_result = await google_drive_service.move_files_to_inbox_on_payment_success(
                    customer_email=user_email,
                    file_ids=file_ids,
                    company_name=None  # Individual users have no company
                )

                moved_count = move_result.get('moved_successfully', 0)
                failed_count = move_result.get('failed_moves', 0)

                logging.info(f"[CONFIRM-INDIVIDUAL] Moved {moved_count}/{len(file_ids)} files successfully")

                if failed_count > 0:
                    logging.warning(f"[CONFIRM-INDIVIDUAL] {failed_count} files failed to move")

                return JSONResponse(
                    status_code=200,
                    content={
                        "success": True,
                        "message": "Individual transaction confirmed successfully",
                        "data": {
                            "transaction_id": request.transaction_id,
                            "square_transaction_id": request.square_transaction_id,
                            "moved_files": moved_count,
                            "failed_files": failed_count,
                            "total_files": len(file_ids)
                        }
                    }
                )

            except Exception as e:
                logging.error(f"[CONFIRM-INDIVIDUAL] File movement failed: {e}", exc_info=True)
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to move files: {str(e)}"
                )

        # ========== CANCEL FLOW: Delete files ==========
        else:
            logging.info(f"[CONFIRM-INDIVIDUAL] Cancelling transaction {request.transaction_id}")

            # Update transaction status to 'cancelled'
            await database.user_transactions.update_one(
                {"transaction_id": request.transaction_id},
                {
                    "$set": {
                        "status": "cancelled",
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            logging.info(f"[CONFIRM-INDIVIDUAL] Updated transaction status to 'cancelled'")

            # Delete files from Temp
            deleted_count = 0
            for file_id in file_ids:
                try:
                    await google_drive_service.delete_file(file_id)
                    deleted_count += 1
                    logging.info(f"[CONFIRM-INDIVIDUAL] Deleted file {file_id}")
                except Exception as e:
                    logging.error(f"[CONFIRM-INDIVIDUAL] Failed to delete file {file_id}: {e}")

            logging.info(f"[CONFIRM-INDIVIDUAL] Deleted {deleted_count}/{len(file_ids)} files")

            return JSONResponse(
                status_code=200,
                content={
                    "success": True,
                    "message": "Individual transaction cancelled, files deleted",
                    "data": {
                        "transaction_id": request.transaction_id,
                        "deleted_files": deleted_count,
                        "total_files": len(file_ids)
                    }
                }
            )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"[CONFIRM-INDIVIDUAL] Unexpected error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )


# Root endpoint
@app.get("/", tags=["Root"])
async def root():
    """Root endpoint with API information."""
    return {
        "name": settings.app_name,
        "version": settings.app_version,
        "status": "healthy",
        "environment": settings.environment,
        "documentation": "/docs" if settings.debug else "Documentation disabled in production",
        "endpoints": {
            "languages": "/api/v1/languages",
            "translate": "/translate",
            "payment_create": "/api/payment/create-intent",
            "payment_success": "/api/payment/success",
            "health": "/health"
        }
    }


# Health check endpoint
@app.get("/health", tags=["Health"])
async def health_check():
    """Health check endpoint for monitoring and load balancers."""
    try:
        from app.database import database

        # Simple MongoDB connection check
        if database.is_connected:
            # Quick ping to verify connection is alive
            await database.client.admin.command('ping')
            return JSONResponse(
                content={
                    "status": "healthy",
                    "database": "connected",
                    "timestamp": time.time()
                },
                status_code=200
            )
        else:
            return JSONResponse(
                content={
                    "status": "unhealthy",
                    "database": "disconnected",
                    "timestamp": time.time()
                },
                status_code=503
            )
    except Exception as e:
        return JSONResponse(
            content={
                "status": "unhealthy",
                "error": str(e),
                "timestamp": time.time()
            },
            status_code=503
        )


# API version endpoint
@app.get("/api/v1", tags=["API Info"])
async def api_info():
    """API version information."""
    return {
        "api_version": "v1",
        "app_version": settings.app_version,
        "features": {},
        "supported_formats": settings.allowed_file_extensions,
        "max_file_size_mb": round(settings.max_file_size / (1024 * 1024), 2)
    }


# Custom exception handlers
from fastapi.exceptions import RequestValidationError

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Handle validation errors with safe encoding handling."""
    # DEBUG: Print validation errors immediately
    print("üî¥ DEBUG: PYDANTIC VALIDATION ERROR!")
    print(f"üî¥ DEBUG: Path: {request.url.path}")
    print(f"üî¥ DEBUG: Errors: {exc.errors()}")
    try:
        # Safely serialize validation errors to avoid Unicode decode issues
        safe_errors = []
        for error in exc.errors():
            safe_error = {}
            for key, value in error.items():
                if isinstance(value, bytes):
                    # Handle bytes values that might contain non-UTF-8 data
                    try:
                        safe_error[key] = value.decode('utf-8')
                    except UnicodeDecodeError:
                        # Try alternative encodings
                        for encoding in ['latin1', 'cp1252', 'iso-8859-1']:
                            try:
                                safe_error[key] = f"<{encoding}> {value.decode(encoding)}"
                                break
                            except UnicodeDecodeError:
                                continue
                        else:
                            safe_error[key] = f"<hex> {value.hex()}"
                elif isinstance(value, str):
                    # Ensure string values are properly handled
                    safe_error[key] = value
                else:
                    safe_error[key] = str(value)
            safe_errors.append(safe_error)

        return JSONResponse(
            status_code=422,
            content={
                "success": False,
                "error": {
                    "code": 422,
                    "message": "Request validation failed",
                    "type": "validation_error",
                    "details": safe_errors
                },
                "timestamp": time.time(),
                "path": str(request.url)
            }
        )
    except Exception as e:
        # Fallback error handling if validation error processing fails
        logging.error(f"Error processing validation error: {e}", exc_info=True)
        return JSONResponse(
            status_code=422,
            content={
                "success": False,
                "error": {
                    "code": 422,
                    "message": "Request validation failed - encoding error",
                    "type": "validation_error_with_encoding_issue",
                    "raw_error": str(exc)
                },
                "timestamp": time.time(),
                "path": str(request.url)
            }
        )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions with consistent error response format."""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "success": False,
            "error": {
                "code": exc.status_code,
                "message": exc.detail,
                "type": "http_error"
            },
            "timestamp": time.time(),
            "path": str(request.url)
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle general exceptions with error logging."""
    logging.error(f"Unhandled exception: {exc}", exc_info=True)

    if settings.debug:
        error_detail = str(exc)
    else:
        error_detail = "Internal server error"

    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": {
                "code": 500,
                "message": error_detail,
                "type": "internal_error"
            },
            "timestamp": time.time(),
            "path": str(request.url)
        }
    )


# ============================================================================
# TIMEOUT MIDDLEWARE - MUST BE OUTERMOST TO CATCH ALL TIMEOUTS
# ============================================================================
# This middleware is added via decorator AFTER all add_middleware calls,
# making it the outermost layer that executes first and catches timeouts.
#
# CRITICAL FIX: Timeout responses must manually add CORS headers because
# they bypass the CORSMiddleware by being created at the outermost layer.
# ============================================================================
@app.middleware("http")
async def timeout_middleware(request: Request, call_next):
    """Add request timeout handling with proper CORS headers."""
    import asyncio
    import time as time_module

    # DETAILED TIMING LOG
    start_time = time_module.time()
    print(f"[TIMEOUT MIDDLEWARE] START: {request.method} {request.url.path}")

    try:
        # Set timeout based on endpoint
        if "/files/upload" in str(request.url):
            timeout = 300  # 5 minutes for file uploads
        elif "/translate" in str(request.url):
            timeout = 120  # 2 minutes for translations
        elif "/api/payment/" in str(request.url):
            timeout = 90   # 90 seconds for payment processing (Google Drive operations)
        elif "/login/" in str(request.url):
            timeout = 60   # 1 minute for authentication
        else:
            timeout = 30   # 30 seconds for other requests

        print(f"[TIMEOUT MIDDLEWARE] Timeout set to {timeout}s, calling next middleware...")
        response = await asyncio.wait_for(call_next(request), timeout=timeout)
        elapsed = time_module.time() - start_time
        print(f"[TIMEOUT MIDDLEWARE] COMPLETE: {elapsed:.2f}s - {request.method} {request.url.path}")
        return response

    except asyncio.TimeoutError:
        elapsed = time_module.time() - start_time
        print(f"[TIMEOUT MIDDLEWARE] TIMEOUT FIRED: {elapsed:.2f}s - {request.method} {request.url.path}")
        # CRITICAL: Manually add CORS headers to timeout responses
        # because they bypass CORSMiddleware
        response = JSONResponse(
            status_code=408,
            content={
                "success": False,
                "error": {
                    "code": 408,
                    "message": "Request timeout",
                    "type": "timeout_error"
                },
                "timestamp": time.time()
            }
        )

        # Add CORS headers manually using settings
        origin = request.headers.get("origin")
        if origin:
            # Check if origin is allowed
            allowed_origins = settings.cors_origins
            if origin in allowed_origins or "*" in allowed_origins:
                response.headers["Access-Control-Allow-Origin"] = origin
                response.headers["Access-Control-Allow-Credentials"] = str(settings.cors_credentials).lower()
                response.headers["Access-Control-Allow-Methods"] = ", ".join(settings.cors_methods)
                response.headers["Access-Control-Allow-Headers"] = "*"
                response.headers["Access-Control-Expose-Headers"] = "X-Request-ID, X-Process-Time"

        logging.warning(f"Request timeout: {request.method} {request.url.path} (>{timeout}s)")
        return response


# Custom OpenAPI schema
def custom_openapi():
    """Generate custom OpenAPI schema with additional information."""
    if app.openapi_schema:
        return app.openapi_schema

    openapi_schema = get_openapi(
        title=settings.app_name,
        version=settings.app_version,
        description="""
        ## Translation Web Server API

        A comprehensive translation service supporting multiple languages, file formats, and payment processing.

        ### Features
        - **Text Translation**: Translate text strings using multiple services
        - **File Translation**: Support for various document formats
        - **Batch Processing**: Handle multiple translations simultaneously
        - **Language Detection**: Automatic language identification
        - **Payment Integration**: Stripe-powered payment processing
        - **Rate Limiting**: Built-in request rate limiting
        - **File Upload**: Secure file handling and storage

        ### Supported Services
        - Google Translate (Free & Paid)
        - DeepL
        - Azure Translator

        ### Authentication
        API key authentication required for production usage.

        ### Rate Limits
        - Free tier: 100 requests per hour
        - Paid tiers: Higher limits based on subscription
        """,
        routes=app.routes,
    )

    # Add security schemes
    openapi_schema["components"]["securitySchemes"] = {
        "ApiKeyAuth": {
            "type": "apiKey",
            "in": "header",
            "name": "X-API-Key"
        },
        "BearerAuth": {
            "type": "http",
            "scheme": "bearer"
        }
    }

    # Add example servers
    openapi_schema["servers"] = [
        {
            "url": f"http://localhost:{settings.port}",
            "description": "Development server"
        },
        {
            "url": "https://api.translator.example.com",
            "description": "Production server"
        }
    ]

    app.openapi_schema = openapi_schema
    return app.openapi_schema


app.openapi = custom_openapi


# Startup and shutdown functions
async def initialize_services():
    """Initialize application services."""
    logging.info("Initializing services...")

    # Initialize MongoDB database connection
    from app.database import database
    mongodb_connected = await database.connect()
    if mongodb_connected:
        logging.info("MongoDB database connection established")
    else:
        logging.warning("MongoDB database connection failed - continuing without database")

    # Initialize translation services
    from app.services.translation_service import translation_service
    logging.info(f"Translation services available: {list(translation_service.services.keys())}")

    # Initialize payment service
    from app.services.payment_service import payment_service
    if payment_service.stripe_enabled:
        logging.info("Stripe payment service initialized")
    else:
        logging.warning("Stripe payment service not configured")

    # Initialize file service
    try:
        from app.services.file_service import file_service
        logging.info("File service initialized")
    except ImportError:
        logging.info("File service not available (stub implementation)")

    logging.info("All services initialized successfully")


async def cleanup_services():
    """Cleanup services on shutdown."""
    logging.info("Cleaning up services...")

    # Disconnect from MongoDB
    from app.database import database
    await database.disconnect()

    # Cleanup temporary files would go here in real implementation
    logging.info("No cleanup needed in stub implementation")

    logging.info("Service cleanup completed")


# Development server runner
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=getattr(logging, settings.log_level.upper()),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # Run the server
    uvicorn.run(
        "app.main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug,
        log_level=settings.log_level.lower(),
        access_log=True
    )
